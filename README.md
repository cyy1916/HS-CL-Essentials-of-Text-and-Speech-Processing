# HS-CL-Essentials-of-Text-and-Speech-Processing

Medical chatbots are increasingly used to improve access to healthcare information and facilitate patient communication. However, different chatbots vary significantly in their architectures, training, and datasets, leading to differences in their performance. This project aims to conduct a comparative analysis of three medical question-answering models: BioBERT-Large, BioGPT-Large, Medical-Llama3-8B.

We used the chatbot_medical_fr dataset to evaluate models. We input both the original patient consultations to doctors and their simplified versions from the dataset into the model, comparing the outputs with the doctors' answers in the dataset. A series of evaluation metrics, including accuracy, response time, fluency, and readability, are used to test the overall performance of the model. Our analysis will reveal the model's strengths, weaknesses, and potential areas for improvement. Through this analysis, we aim to provide valuable insights into the current state of conversational AI in healthcare.
